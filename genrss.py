#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç”Ÿæˆ RSS feed å·¥å…·
æ ¹æ® ai/index.html ä¸­æœ€è¿‘30å¤©çš„é“¾æ¥ç”Ÿæˆ RSS feed
ä¿å­˜ä¸º feed.xml
"""

import os
import re
import argparse
from urllib.parse import urlparse, urljoin
from datetime import datetime, timedelta
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
from html.parser import HTMLParser


class LinkExtractor(HTMLParser):
    """è‡ªå®šä¹‰HTMLè§£æå™¨ï¼Œæå–é“¾æ¥å’Œå®Œæ•´çš„<li>å†…å®¹"""
    
    def __init__(self):
        super().__init__()
        self.links = []
        self.current_tag = None
        self.current_attrs = None
        self.current_text = ""
        self.li_content = ""
        self.in_li = False
        self.current_href = None
        
    def handle_starttag(self, tag, attrs):
        if tag == 'li':
            self.in_li = True
            self.li_content = ""
            self.current_href = None
        elif tag == 'a' and self.in_li:
            attrs_dict = dict(attrs)
            if 'href' in attrs_dict:
                self.current_href = attrs_dict['href']
        
        self.current_tag = tag
        self.current_attrs = dict(attrs)
        self.current_text = ""
        
    def handle_endtag(self, tag):
        if tag == 'li' and self.in_li:
            if self.current_href and self.current_href.startswith('http'):
                self.links.append({
                    'url': self.current_href,
                    'text': self.li_content.strip(),
                    'tag': None
                })
            self.in_li = False
            self.li_content = ""
            self.current_href = None
        
        self.current_tag = None
        self.current_attrs = None
        self.current_text = ""
        
    def handle_data(self, data):
        if self.in_li:
            self.li_content += data


def parse_html_file(file_path, domain_filter=None):
    """è§£æ HTML æ–‡ä»¶ï¼Œæå–æ‰€æœ‰é“¾æ¥"""
    print(f"æ­£åœ¨è§£ææ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨è‡ªå®šä¹‰è§£æå™¨
    parser = LinkExtractor()
    parser.feed(content)
    
    # è¿‡æ»¤é“¾æ¥
    links = []
    for link in parser.links:
        href = link['url']
        # å¦‚æœæŒ‡å®šäº†åŸŸåè¿‡æ»¤å™¨ï¼Œåªå¤„ç†åŒ¹é…çš„åŸŸå
        if domain_filter:
            if domain_filter in href:
                links.append(link)
        else:
            # ä¸è¿‡æ»¤ï¼Œå¤„ç†æ‰€æœ‰HTTPé“¾æ¥
            links.append(link)
    
    print(f"æ‰¾åˆ° {len(links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
    return links


def extract_date_from_text(text):
    """ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸä¿¡æ¯ï¼Œè¿”å› datetime å¯¹è±¡"""
    # åŒ¹é…æ ¼å¼ï¼šï¼ˆ2025-07-21 14:33ï¼‰ä½¿ç”¨ä¸­æ–‡å…¨è§’æ‹¬å·
    date_pattern = r'ï¼ˆ(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2})ï¼‰'
    match = re.search(date_pattern, text)
    if match:
        date_str = match.group(1)
        time_str = match.group(2)
        try:
            return datetime.strptime(f"{date_str} {time_str}", '%Y-%m-%d %H:%M')
        except ValueError:
            return None
    return None


def filter_recent_links(links, days=30):
    """è¿‡æ»¤æœ€è¿‘æŒ‡å®šå¤©æ•°çš„é“¾æ¥"""
    cutoff_date = datetime.now() - timedelta(days=days)
    recent_links = []
    
    for link in links:
        date_obj = extract_date_from_text(link['text'])
        if date_obj and date_obj >= cutoff_date:
            link['date'] = date_obj
            recent_links.append(link)
    
    # æŒ‰æ—¥æœŸæ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
    recent_links.sort(key=lambda x: x['date'], reverse=True)
    
    print(f"æ‰¾åˆ°æœ€è¿‘ {days} å¤©å†…çš„ {len(recent_links)} ä¸ªé“¾æ¥")
    return recent_links


def filter_links_by_tag(links, tag):
    """æŒ‰æ ‡ç­¾è¿‡æ»¤é“¾æ¥"""
    if not tag:
        return links
    
    filtered_links = []
    for link in links:
        categories = extract_categories_from_text(link['text'])
        if tag.lower() in [cat.lower() for cat in categories]:
            filtered_links.append(link)
    
    print(f"æŒ‰æ ‡ç­¾ '{tag}' è¿‡æ»¤åæ‰¾åˆ° {len(filtered_links)} ä¸ªé“¾æ¥")
    return filtered_links


def extract_title_from_text(text):
    """ä»æ–‡æœ¬ä¸­æå–æ ‡é¢˜ï¼Œå»æ‰æ—¥æœŸå’Œæ ‡ç­¾"""
    # å»æ‰æ—¥æœŸéƒ¨åˆ†ï¼šï¼ˆ2025-07-21 14:33ï¼‰
    text = re.sub(r'ï¼ˆ\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}ï¼‰', '', text)
    # å»æ‰å¼€å¤´çš„æ ‡ç­¾ï¼š[ai][blog]
    text = re.sub(r'^\[[^\]]+\]', '', text)
    # å»æ‰å¤šä¸ªè¿ç»­çš„æ ‡ç­¾
    text = re.sub(r'^\[[^\]]+\]\[[^\]]+\]', '', text)
    text = re.sub(r'^\[[^\]]+\]\[[^\]]+\]\[[^\]]+\]', '', text)
    return text.strip()


def extract_categories_from_text(text):
    """ä»æ–‡æœ¬ä¸­æå–åˆ†ç±»æ ‡ç­¾"""
    categories = re.findall(r'\[([^\]]+)\]', text)
    return categories


def generate_rss(links, base_url="https://www.laobu.com", site_title="è€å¸ƒçš„AIçŸ¥è¯†åº“", site_description="è®°å½•AIç›¸å…³çš„å­¦ä¹ å’Œæ€è€ƒ"):
    """ç”Ÿæˆ RSS feed XML"""
    print("æ­£åœ¨ç”Ÿæˆ RSS feed...")
    
    # åˆ›å»ºæ ¹å…ƒç´ 
    rss = Element('rss')
    rss.set('xmlns:dc', 'http://purl.org/dc/elements/1.1/')
    rss.set('xmlns:content', 'http://purl.org/rss/1.0/modules/content/')
    rss.set('xmlns:atom', 'http://www.w3.org/2005/Atom')
    rss.set('xmlns:media', 'http://search.yahoo.com/mrss/')
    rss.set('version', '2.0')
    
    # åˆ›å»º channel å…ƒç´ 
    channel = SubElement(rss, 'channel')
    
    # åŸºæœ¬ä¿¡æ¯
    SubElement(channel, 'title').text = site_title
    SubElement(channel, 'link').text = base_url + '/ai/index.html'
    SubElement(channel, 'description').text = site_description
    SubElement(channel, 'language').text = 'zh-CN'
    SubElement(channel, 'lastBuildDate').text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S +0000')
    SubElement(channel, 'generator').text = 'genrss.py'
    
    # æ·»åŠ  atom:link
    atom_link = SubElement(channel, 'atom:link')
    atom_link.set('href', f'{base_url}/feed.xml')
    atom_link.set('rel', 'self')
    atom_link.set('type', 'application/rss+xml')
    
    # æ·»åŠ æ–‡ç« é¡¹ç›®
    for link in links:
        item = SubElement(channel, 'item')
        
        # æ ‡é¢˜
        title = extract_title_from_text(link['text'])
        SubElement(item, 'title').text = title
        
        # é“¾æ¥
        SubElement(item, 'link').text = link['url']
        SubElement(item, 'guid').text = link['url']
        
        # æè¿° - ä½¿ç”¨å®Œæ•´æ–‡æœ¬ä½œä¸ºæè¿°
        SubElement(item, 'description').text = link['text']
        
        # å‘å¸ƒæ—¥æœŸ
        if 'date' in link:
            # RSS æ—¥æœŸæ ¼å¼ï¼šWed, 02 Oct 2002 15:00:00 +0000
            pub_date = link['date'].strftime('%a, %d %b %Y %H:%M:%S +0000')
            SubElement(item, 'pubDate').text = pub_date
        
        # åˆ†ç±»
        categories = extract_categories_from_text(link['text'])
        for category in categories:
            cat_elem = SubElement(item, 'category')
            cat_elem.text = category
    
    return rss


def format_xml(element):
    """æ ¼å¼åŒ– XML è¾“å‡º"""
    rough_string = tostring(element, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent="  ", encoding='utf-8').decode('utf-8')


def auto_detect_base_url(links):
    """ä»é“¾æ¥ä¸­è‡ªåŠ¨æ£€æµ‹åŸºç¡€åŸŸå"""
    domain_counts = {}
    
    for link in links:
        url = link['url']
        parsed = urlparse(url)
        domain = f"{parsed.scheme}://{parsed.netloc}"
        domain_counts[domain] = domain_counts.get(domain, 0) + 1
    
    if domain_counts:
        # è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„åŸŸå
        most_common_domain = max(domain_counts.items(), key=lambda x: x[1])[0]
        return most_common_domain
    
    return "https://www.laobu.com"  # é»˜è®¤åŸŸå


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”Ÿæˆ RSS feed')
    parser.add_argument('-u', '--url', '--base-url', 
                      help='æŒ‡å®šåŸºç¡€URL (ä¾‹å¦‚: https://www.laobu.com)')
    parser.add_argument('-i', '--input', default='ai/index.html',
                      help='è¾“å…¥HTMLæ–‡ä»¶è·¯å¾„ (é»˜è®¤: ai/index.html)')
    parser.add_argument('-o', '--output', default='feed.xml',
                      help='è¾“å‡ºRSSæ–‡ä»¶è·¯å¾„ (é»˜è®¤: feed.xml)')
    parser.add_argument('-d', '--days', type=int, default=30,
                      help='åŒ…å«æœ€è¿‘å¤šå°‘å¤©çš„æ–‡ç«  (é»˜è®¤: 30)')
    parser.add_argument('-t', '--tag',
                      help='æŒ‰æ ‡ç­¾è¿‡æ»¤æ–‡ç« ï¼Œä¾‹å¦‚ -t ai æˆ– -t bushcraft')
    parser.add_argument('--title', default='è€å¸ƒçš„AIçŸ¥è¯†åº“',
                      help='RSS feed æ ‡é¢˜')
    parser.add_argument('--description', default='è®°å½•AIç›¸å…³çš„å­¦ä¹ å’Œæ€è€ƒ',
                      help='RSS feed æè¿°')
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†æ ‡ç­¾ä½†æ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨è®¾ç½®è¾“å‡ºæ–‡ä»¶å
    if args.tag and args.output == 'feed.xml':
        args.output = f'feed-{args.tag}.xml'
    
    print("=== RSS Feed ç”Ÿæˆå™¨ ===")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {args.input}")
        return
    
    try:
        # è§£æ HTML æ–‡ä»¶
        all_links = parse_html_file(args.input)
        
        if not all_links:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆé“¾æ¥")
            return
        
        # ç¡®å®šåŸºç¡€URL
        if args.url:
            base_url = args.url.rstrip('/')
            print(f"ğŸŒ ä½¿ç”¨æŒ‡å®šçš„åŸºç¡€URL: {base_url}")
        else:
            base_url = auto_detect_base_url(all_links)
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°åŸºç¡€URL: {base_url}")
        
        # è¿‡æ»¤æœ€è¿‘çš„é“¾æ¥
        recent_links = filter_recent_links(all_links, args.days)
        
        if not recent_links:
            print(f"âš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ€è¿‘ {args.days} å¤©å†…çš„é“¾æ¥")
            return
        
        # æŒ‰æ ‡ç­¾è¿‡æ»¤é“¾æ¥
        if args.tag:
            recent_links = filter_links_by_tag(recent_links, args.tag)
            if not recent_links:
                print(f"âš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ä¸º '{args.tag}' çš„é“¾æ¥")
                return
        
        # ç”Ÿæˆ RSS feed
        rss = generate_rss(recent_links, base_url, args.title, args.description)
        
        # æ ¼å¼åŒ–å¹¶ä¿å­˜ XML
        xml_content = format_xml(rss)
        
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(xml_content)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {args.output}")
        print(f"ğŸ“Š åŒ…å«æœ€è¿‘ {args.days} å¤©å†…çš„ {len(recent_links)} ç¯‡æ–‡ç« ")
        
        # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ  åŸºç¡€åŸŸå: {base_url}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {recent_links[-1]['date'].strftime('%Y-%m-%d')} åˆ° {recent_links[0]['date'].strftime('%Y-%m-%d')}")
        
        print("\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
        tag_counts = {}
        for link in recent_links:
            categories = extract_categories_from_text(link['text'])
            for category in categories:
                tag_counts[category] = tag_counts.get(category, 0) + 1
        
        for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  [{tag}]: {count} ç¯‡")
        
        print(f"\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print(f"  # ç”Ÿæˆæœ€è¿‘7å¤©çš„RSS:")
        print(f"  python3 genrss.py -d 7 -o feed-weekly.xml")
        print(f"  # è‡ªå®šä¹‰æ ‡é¢˜å’Œæè¿°:")
        print(f"  python3 genrss.py -t 'è€å¸ƒAIåšå®¢' --description 'AIå­¦ä¹ ç¬”è®°ä¸æ€è€ƒ'")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()