#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç”Ÿæˆ sitemap.xml å·¥å…·
æ ¹æ® ai/index.html ä¸­çš„æ‰€æœ‰é“¾æ¥ç”Ÿæˆç½‘ç«™åœ°å›¾
"""

import os
import re
import argparse
from urllib.parse import urlparse, urljoin
from datetime import datetime
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
from html.parser import HTMLParser


class LinkExtractor(HTMLParser):
    """è‡ªå®šä¹‰HTMLè§£æå™¨ï¼Œæå–é“¾æ¥"""
    
    def __init__(self):
        super().__init__()
        self.links = []
        self.current_tag = None
        self.current_attrs = None
        self.current_text = ""
        
    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        self.current_attrs = dict(attrs)
        self.current_text = ""
        
    def handle_endtag(self, tag):
        if tag == 'a' and self.current_attrs and 'href' in self.current_attrs:
            href = self.current_attrs['href']
            text = self.current_text.strip()
            if href.startswith('http'):
                self.links.append({
                    'url': href,
                    'text': text,
                    'tag': None  # ä¿æŒå…¼å®¹æ€§
                })
        self.current_tag = None
        self.current_attrs = None
        self.current_text = ""
        
    def handle_data(self, data):
        if self.current_tag == 'a':
            self.current_text += data


def parse_html_file(file_path, domain_filter=None):
    """è§£æ HTML æ–‡ä»¶ï¼Œæå–æ‰€æœ‰é“¾æ¥"""
    print(f"æ­£åœ¨è§£ææ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨è‡ªå®šä¹‰è§£æå™¨
    parser = LinkExtractor()
    parser.feed(content)
    
    # è¿‡æ»¤é“¾æ¥
    links = []
    for link in parser.links:
        href = link['url']
        # å¦‚æœæŒ‡å®šäº†åŸŸåè¿‡æ»¤å™¨ï¼Œåªå¤„ç†åŒ¹é…çš„åŸŸå
        if domain_filter:
            if domain_filter in href:
                links.append(link)
        else:
            # ä¸è¿‡æ»¤ï¼Œå¤„ç†æ‰€æœ‰HTTPé“¾æ¥
            links.append(link)
    
    print(f"æ‰¾åˆ° {len(links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
    return links


def extract_date_from_text(text):
    """ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸä¿¡æ¯"""
    # åŒ¹é…æ ¼å¼ï¼šï¼ˆ2025-07-21 14:33ï¼‰
    date_pattern = r'ï¼ˆ(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}ï¼‰'
    match = re.search(date_pattern, text)
    if match:
        return match.group(1)
    return None


def get_priority_by_tag(url, text):
    """æ ¹æ®æ ‡ç­¾å’Œå†…å®¹ç¡®å®šä¼˜å…ˆçº§"""
    # é»˜è®¤ä¼˜å…ˆçº§
    priority = 0.5
    
    # åšå®¢æ–‡ç« ä¼˜å…ˆçº§æ›´é«˜
    if '[blog]' in text:
        priority = 0.9
    elif '[ai]' in text:
        priority = 0.8
    elif '[bushcraft]' in text or '[mind]' in text:
        priority = 0.7
    elif '[se]' in text:
        priority = 0.6
    
    return priority


def get_change_frequency(url, text):
    """æ ¹æ®å†…å®¹ç±»å‹ç¡®å®šæ›´æ–°é¢‘ç‡"""
    if '[blog]' in text:
        return 'weekly'
    elif 'index.html' in url or 'tags.html' in url:
        return 'daily'
    else:
        return 'monthly'


def generate_sitemap(links, base_url="https://www.xiaobu.net"):
    """ç”Ÿæˆ sitemap.xml"""
    print("æ­£åœ¨ç”Ÿæˆ sitemap.xml...")
    
    # åˆ›å»ºæ ¹å…ƒç´ 
    urlset = Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    
    # æ·»åŠ é¦–é¡µ
    url_elem = SubElement(urlset, 'url')
    SubElement(url_elem, 'loc').text = base_url
    SubElement(url_elem, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
    SubElement(url_elem, 'changefreq').text = 'daily'
    SubElement(url_elem, 'priority').text = '1.0'
    
    # æ·»åŠ  ai/index.html
    url_elem = SubElement(urlset, 'url')
    SubElement(url_elem, 'loc').text = f"{base_url}/ai/index.html"
    SubElement(url_elem, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
    SubElement(url_elem, 'changefreq').text = 'daily'
    SubElement(url_elem, 'priority').text = '0.9'
    
    # å¤„ç†æ‰€æœ‰é“¾æ¥
    processed_urls = set()  # å»é‡
    
    for link in links:
        url = link['url']
        text = link['text']
        
        # å»é‡å¤„ç†
        if url in processed_urls:
            continue
        processed_urls.add(url)
        
        # åˆ›å»º URL å…ƒç´ 
        url_elem = SubElement(urlset, 'url')
        SubElement(url_elem, 'loc').text = url
        
        # å°è¯•ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸ
        date_str = extract_date_from_text(text)
        if date_str:
            SubElement(url_elem, 'lastmod').text = date_str
        else:
            # ä½¿ç”¨å½“å‰æ—¥æœŸ
            SubElement(url_elem, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
        
        # è®¾ç½®æ›´æ–°é¢‘ç‡
        changefreq = get_change_frequency(url, text)
        SubElement(url_elem, 'changefreq').text = changefreq
        
        # è®¾ç½®ä¼˜å…ˆçº§
        priority = get_priority_by_tag(url, text)
        SubElement(url_elem, 'priority').text = str(priority)
    
    return urlset


def format_xml(element):
    """æ ¼å¼åŒ– XML è¾“å‡º"""
    rough_string = tostring(element, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent="  ", encoding='utf-8').decode('utf-8')


def auto_detect_base_url(links):
    """ä»é“¾æ¥ä¸­è‡ªåŠ¨æ£€æµ‹åŸºç¡€åŸŸå"""
    domain_counts = {}
    
    for link in links:
        url = link['url']
        parsed = urlparse(url)
        domain = f"{parsed.scheme}://{parsed.netloc}"
        domain_counts[domain] = domain_counts.get(domain, 0) + 1
    
    if domain_counts:
        # è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„åŸŸå
        most_common_domain = max(domain_counts.items(), key=lambda x: x[1])[0]
        return most_common_domain
    
    return "https://www.xiaobu.net"  # é»˜è®¤åŸŸå


def replace_domain_in_links(links, new_base_url):
    """å°†é“¾æ¥ä¸­çš„åŸŸåæ›¿æ¢ä¸ºæ–°çš„åŸºç¡€URLåŸŸå"""
    new_parsed = urlparse(new_base_url)
    new_netloc = new_parsed.netloc
    new_scheme = new_parsed.scheme
    
    replaced_links = []
    for link in links:
        old_url = link['url']
        old_parsed = urlparse(old_url)
        
        # æ„å»ºæ–°çš„URLï¼ˆä¿æŒè·¯å¾„å’ŒæŸ¥è¯¢å‚æ•°ï¼‰
        new_url = f"{new_scheme}://{new_netloc}{old_parsed.path}"
        if old_parsed.query:
            new_url += f"?{old_parsed.query}"
        if old_parsed.fragment:
            new_url += f"#{old_parsed.fragment}"
        
        # åˆ›å»ºæ–°çš„é“¾æ¥å¯¹è±¡
        new_link = link.copy()
        new_link['url'] = new_url
        replaced_links.append(new_link)
    
    return replaced_links


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”Ÿæˆç½‘ç«™åœ°å›¾ sitemap.xml')
    parser.add_argument('-u', '--url', '--base-url', 
                      help='æŒ‡å®šåŸºç¡€URL (ä¾‹å¦‚: https://www.xiaobu.net)')
    parser.add_argument('-i', '--input', default='ai/index.html',
                      help='è¾“å…¥HTMLæ–‡ä»¶è·¯å¾„ (é»˜è®¤: ai/index.html)')
    parser.add_argument('-o', '--output', default='sitemap.xml',
                      help='è¾“å‡ºXMLæ–‡ä»¶è·¯å¾„ (é»˜è®¤: sitemap.xml)')
    parser.add_argument('--auto-detect', action='store_true',
                      help='è‡ªåŠ¨æ£€æµ‹åŸºç¡€URLï¼ˆä»HTMLæ–‡ä»¶ä¸­çš„é“¾æ¥ï¼‰')
    parser.add_argument('--replace-domain', action='store_true',
                      help='æ›¿æ¢é“¾æ¥ä¸­çš„åŸŸåä¸ºæŒ‡å®šçš„åŸºç¡€URLåŸŸåï¼ˆè€Œä¸æ˜¯è¿‡æ»¤é“¾æ¥ï¼‰')
    
    args = parser.parse_args()
    
    print("=== ç½‘ç«™åœ°å›¾ç”Ÿæˆå™¨ ===")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {args.input}")
        return
    
    try:
        # è§£æ HTML æ–‡ä»¶
        all_links = parse_html_file(args.input)
        
        if not all_links:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆé“¾æ¥")
            return
        
        # ç¡®å®šåŸºç¡€URLå’Œå¤„ç†é“¾æ¥
        if args.url:
            base_url = args.url.rstrip('/')  # ç§»é™¤æœ«å°¾çš„æ–œæ 
            print(f"ğŸŒ ä½¿ç”¨æŒ‡å®šçš„åŸºç¡€URL: {base_url}")
            
            if args.replace_domain:
                # æ›¿æ¢æ‰€æœ‰é“¾æ¥çš„åŸŸå
                links = replace_domain_in_links(all_links, base_url)
                print(f"ğŸ”„ å·²å°† {len(links)} ä¸ªé“¾æ¥çš„åŸŸåæ›¿æ¢ä¸º: {urlparse(base_url).netloc}")
            else:
                # è¿‡æ»¤å‡ºåŒ¹é…æŒ‡å®šåŸŸåçš„é“¾æ¥
                parsed_base = urlparse(base_url)
                domain_to_filter = parsed_base.netloc
                links = [link for link in all_links if domain_to_filter in link['url']]
                print(f"ğŸ” è¿‡æ»¤åæ‰¾åˆ° {len(links)} ä¸ªåŒ¹é…åŸŸåçš„é“¾æ¥")
                
                # æ£€æŸ¥è¿‡æ»¤åæ˜¯å¦è¿˜æœ‰é“¾æ¥
                if not links:
                    print(f"âš ï¸  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°åŒ¹é…åŸŸå '{parsed_base.netloc}' çš„é“¾æ¥")
                    print(f"ğŸ“‹ æ‰€æœ‰æ‰¾åˆ°çš„åŸŸå:")
                    domains = set()
                    for link in all_links:
                        parsed = urlparse(link['url'])
                        domains.add(parsed.netloc)
                    for domain in sorted(domains):
                        print(f"  - {domain}")
                    print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ --replace-domain å‚æ•°å¯ä»¥æ›¿æ¢åŸŸåè€Œä¸æ˜¯è¿‡æ»¤")
                    return
        else:
            # è‡ªåŠ¨æ£€æµ‹åŸŸå
            base_url = auto_detect_base_url(all_links)
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°åŸºç¡€URL: {base_url}")
            print("ğŸ’¡ æç¤º: å¯ä»¥ä½¿ç”¨ -u å‚æ•°æŒ‡å®šè‡ªå®šä¹‰åŸŸå")
            # ä½¿ç”¨æ£€æµ‹åˆ°çš„åŸŸåï¼Œä¸è¿›è¡Œè¿‡æ»¤
            links = all_links
        
        # ç”Ÿæˆ sitemap
        sitemap = generate_sitemap(links, base_url)
        
        # æ ¼å¼åŒ–å¹¶ä¿å­˜ XML
        xml_content = format_xml(sitemap)
        
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(xml_content)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {args.output}")
        print(f"ğŸ“Š åŒ…å« {len(links) + 2} ä¸ª URLï¼ˆåŒ…æ‹¬é¦–é¡µå’Œç´¢å¼•é¡µï¼‰")
        
        # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ  åŸºç¡€åŸŸå: {base_url}")
        print("\nğŸ“ˆ é“¾æ¥ç»Ÿè®¡:")
        tag_counts = {}
        for link in links:
            text = link['text']
            # æå–æ ‡ç­¾
            tags = re.findall(r'\[([^\]]+)\]', text)
            for tag in tags:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  [{tag}]: {count} ç¯‡")
            
        # æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
        print(f"\nğŸ’¡ å¤šåŸŸåä½¿ç”¨ç¤ºä¾‹:")
        print(f"  # è¿‡æ»¤ç°æœ‰åŸŸåçš„é“¾æ¥ï¼š")
        print(f"  python3 gensitemap.py -u https://www.laobu.net")
        print(f"  # æ›¿æ¢ä¸ºæ–°åŸŸåï¼š")
        print(f"  python3 gensitemap.py -u https://blog.laobu.net --replace-domain -o sitemap-blog.xml")
        print(f"  python3 gensitemap.py -u https://docs.laobu.net --replace-domain -o sitemap-docs.xml")  
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main() 